---
title: 'DGM: A deep learning algorithm for solving partial differential equations'
date: 2023-11-17
permalink: /posts/dgm
excerpt: "A blog post on the paper <i>'DGM: A deep learning algorithm for solving partial differential equations'</i> by Justin Sirignano and Konstantinos Spiliopoulos, published in 2018."
categories: 
  - Blog Posts
tags:
  - Deep Learning 
  - Machine Learning
  - Partial Differential Equation
---

[Version 1]

Partial Differential Equations (PDEs) play a crucial role in modeling various phenomena in physics, engineering, and finance. Solving high-dimensional PDEs has been a longstanding computational challenge, with traditional methods becoming impractical due to the explosion in the number of grid points and the demand for reduced time step size.

In this blog post, I explore the approach from the paper *'DGM: A deep learning algorithm for solving partial differential equations'* by Justin Sirignano and Konstantinos Spiliopoulos, published in 2018 called the Deep Galerkin Method (DGM) that leverages deep learning to efficiently solve high-dimensional PDEs. 

## TL;DR
High-dimensional PDEs, with multiple spatial dimensions, pose a computational challenge due to the impracticality of using traditional mesh-based finite difference methods. The mesh size grows exponentially with the number of dimensions, making it computationally intractable. The paper proposes a mesh-free approach using a deep neural network to approximate a solution, mitigating the challenges associated with traditional methods.

The Deep Galerkin Method (DGM) is a mesh-free algorithm inspired by the Galerkin method, a widely-used computational technique for solving PDEs. However, instead of representing the solution as a linear combination of basis functions, the DGM employs a deep neural network. The neural network is trained to satisfy the differential operator, initial conditions, and boundary conditions using stochastic gradient descent on randomly sampled spatial points.

The authors present promising numerical results for the DGM on a class of high-dimensional free boundary PDEs, demonstrating its accuracy in solving problems up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE and Burgers' equation, showcasing its versatility and robustness.

To address the computational cost associated with calculating second derivatives in higher dimensions, the paper introduces a modified algorithm using a Monte Carlo method. This approach significantly reduces computational expenses while introducing some bias and variance. The trade-off between computational efficiency and accuracy is carefully considered.

In conclusion, by combining insights from machine learning and numerical methods, the Deep Galerkin Method opens new possibilities for tackling complex high-dimensional PDEs in various scientific and engineering domains.

## Introduction
Addressing the computational hurdles posed by high-dimensional Partial Differential Equations (PDEs) that involve multiple spatial dimensions is a challenging task. Traditional mesh-based finite difference methods become impractical in such scenarios, as the mesh size grows exponentially with the increasing number of dimensions, rendering computations intractable. In response to this challenge, the paper introduces an innovative solution that embraces a mesh-free approach, leveraging the power of deep neural networks to approximate solutions and alleviate the limitations associated with conventional methods.

At the core of this approach is the Deep Galerkin Method (DGM), a mesh-free algorithm inspired by the well-established Galerkin method, widely employed in computational techniques for solving PDEs. Diverging from the conventional representation of the solution as a linear combination of basis functions, the DGM takes a novel path by incorporating a deep neural network into its framework. This neural network undergoes training to conform to the requirements of the differential operator as well as the initial and boundary conditions through stochastic gradient descent on randomly sampled time and spatial points. This training process enhances the DGM's ability to provide accurate solutions while circumventing the computational challenges associated with traditional methodologies.

In this blog post I will first explain all the necessarry background information that is necessary to understand to needed concepts. Together, we will have a look at
- Partial Differential Equation
- (Stochastic) Gradient Descent
- Antithetic Sampling

Then I will show the algorithm presented in the paper as well as modification to it in order to increase the computation speed. And finally, I will have a look on the experimental results that were made by the authors.

## Partial Differential Equations
Partial Differential Equations (PDEs) stand as the backbone of mathematical modeling, governing phenomena from heat transfer and fluid dynamics to quantum mechanics. These intricate mathematical entities have been pivotal in our quest to understand and predict the behavior of complex systems.

Partial Differential Equations involve functions of multiple variables and their partial derivatives. They play a pivotal role in describing how physical quantities vary with respect to multiple independent variables, such as space and time. The general form of a PDE can be expressed as:

$$F(t, x_1, ..., x_n, u, \frac{\partial u}{\partial t}, \frac{\partial u}{\partial x_1}, ..., \frac{\partial u}{\partial x_n}, \frac{\partial^2 u}{\partial x_{1}^2}, ..., \frac{\partial^2 u}{\partial x_{n}^2},...) = 0$$

Here, $u$ represents the unknown function, and $\frac{\partial u}{\partial t}$ and $\frac{\partial u}{\partial x_i}$​ denote its partial derivatives with respect to time and each spatial variable $x_i$​, respectively.

Partial Differential Equations (PDEs) can be classified into several categories based on their characteristics and mathematical properties. For once, we can classify them into linear and nonlinear PDEs:
- **Linear PDEs**: The dependent variable and its derivatives appear linearly in the equation. Superposition principle holds, meaning if $u_1$​ and $u_2$ are solutions, then $au_1 + bu_2$​ is also a solution.
- **Nonlinear PDEs**: The dependent variable and its derivatives appear nonlinearly. Superposition principle does not generally hold.

We can classify PDEs based on their order:
- **First-Order PDEs**: Involving only first-order partial derivatives.
- **Second-Order PDEs**: Involving second-order partial derivatives.

Elliptic, Hyperbolic, and Parabolic PDEs:
- **Elliptic PDEs**: Typically associated with problems of steady-state equilibrium. Examples include Laplace's equation and Poisson's equation.
- **Hyperbolic PDEs**: Associated with problems involving waves and propagating phenomena. Examples include the wave equation and the advection equation.
- **Parabolic PDEs**: Commonly describe problems involving diffusion and time-dependent processes. Examples include the heat equation and the diffusion equation.

Homogeneous and Nonhomogeneous PDEs:
- **Homogeneous PDEs**: When all terms in the equation are functions of the dependent variable and its derivatives.
- **Nonhomogeneous PDEs**: Include additional source or forcing terms that are functions of the independent variables.

Quasilinear and Fully Nonlinear PDEs:
- **Quasilinear PDEs**: The coefficients of the highest-order derivatives are functions of the dependent variable and lower-order derivatives.
- **Fully Nonlinear PDEs**: The coefficients can depend on both the dependent variable and its derivatives.

In the paper the DGM is applied to a class of quasilinear parabolic PDEs. However, the algorithm can easily be modified to other classes of PDEs.

### Significance of PDEs

In various scientific, engineering, and environmental domains PDEs occupy a central and expansive role making them an indispensable tool for understanding and optimizing complex systems. One of the primary areas where PDEs wield considerable influence is in the modeling of physical phenomena. They provide a robust mathematical framework for describing the evolution of dynamic systems over both time and space. Whether it be heat conduction, fluid flow, electromagnetic fields, or quantum mechanics, PDEs serve as an essential tool for capturing the intricacies of these phenomena and facilitating a deeper comprehension of their behavior.

In the field of engineering and technology, PDEs play a crucial role in optimizing designs and enhancing the performance of diverse systems. Engineers leverage these equations to simulate real-world scenarios, allowing for the refinement of structures such as bridges, aircraft, and electronic devices. The application of PDEs in engineering not only aids in the conceptualization of designs but also enables the prediction of system behavior under various conditions, contributing to the development of more robust and efficient technologies.

Environmental studies represent another domain where the significance of PDEs becomes apparent. In this context, PDEs are instrumental in modeling and understanding complex processes that have far-reaching implications for the planet. Groundwater flow, air pollution dispersion, and climate dynamics are examples of environmental phenomena that can be effectively studied and analyzed using PDEs. By incorporating these equations into environmental models, scientists can gain valuable insights into the behavior of ecosystems and contribute to the development of sustainable solutions.

Furthermore, the interdisciplinary nature of PDEs allows for their application in a wide range of scientific and technological research. Whether it's predicting the behavior of materials at the nanoscale or simulating the dynamics of biological processes, PDEs provide a versatile and powerful tool for researchers across various disciplines.

Hence, PDEs are of significant use across many fields. Their ability to mathematically represent complex phenomena, simulate real-world scenarios, and contribute to the advancement of diverse areas of research underscores their indispensable role in shaping our understanding of the physical world and optimizing technological and environmental processes.

### Challenges in Solving PDEs

PDEs present intricate challenges in practical applications, with two notable aspects contributing to the complexity of their solution: the critical role of boundary and initial conditions and high dimensionality. The accurate solution of PDEs is contingent upon specifying appropriate boundary and initial conditions. These conditions act as essential constraints that define the behavior of the system over time and space. However, in many practical instances, precisely determining these conditions can be a challening task. Uncertainty or lack of exact knowledge about boundary and initial conditions can introduce ambiguity and compromise the reliability of the obtained solutions. This uncertainty poses a significant challenge, forcing researchers and practitioners to develop robust methodologies for dealing with imprecise or incomplete information regarding boundary and initial conditions.


Furthermore, high dimensionality is a prevalent concern in numerous real-world scenarios involving PDEs, where a large number of variables are inherent in the mathematical representation of dynamic systems. This characteristic results in systems that are high-dimensional, posing computational challenges for traditional numerical methods. The computational complexity grows exponentially with the increase in dimensions, making it necessary to explore innovative computational techniques to efficiently handle such situations.

In this blogpost we will explore a deep learning approach that employs a mesh-free algorithm for approximating the solution of a PDE, making it computionally more feasible in higher dimension.

## A note on the Galerkin Method
The Galekin method named after the Russian mathematician Boris Galerkin and developed in 1915 is as a numerical approach to solving partial differential equations. It doesn't attempt to find an exact solution to a problem but instead seeks an approximate solution by projecting it onto a finite-dimensional subspace.

At its core, the Galekin method involves choosing a finite-dimensional space of basis functions and determining the coefficients that minimize the residual in the chosen space. With this approach it is possible to transform a rather challenging differential equation into a easier to solve algebraic problem, allowing for efficient computation and approximation.

The algorithm presented in the paper is called "Deep Galerkin Method" (DGM), since it is similar in spirit to the Galerkin method, in the sense that the algorithm tries to approximate the solution of the PDE. But instead of using of using a linear combination of basis functions the DGM uses a deep neural network to satisfy the differential operator, the initial condition and the boundary conditions of the PDE. 

## On (Stochastic) Gradient Descent
- a

## The algorithm

- Define (general) PDE
- Define deep neural network with parameters $$\theta$$
- Algorithm:
  - Draw random points $$s_n$$
  - Define error function $$G(\theta_n, s_n)$$
  - Take descent steps until convergence: $$\theta_{n+1}= \theta_{n} - \alpha_n \nabla_{\theta}G(\theta_n, s_n)$$

- Show correctness

### Monte Carlo 2nd derivatives

- Problem with computation of second derivatives
- Describe Monte Carlo approach for fast computation of 2nd derivatives
- Show approximation bias, approximation error 
- Describe the modified algorithm

## Numerical Analysis

Here we discuss the numerical results of the algorithm.

The algorithm is tested on
- High-dimensional Free Boundary PDE
- High-dimensional Hamilton-Jacobi-Bellman PDE
- Burgers’ equation

The discussion of the experiments is structured as 
- Define the PDE
- Adjust the algorithm to the specific problem
- Implementation details
- Results

## Neural Network Approximation Theorem for PDEs

In progress

The authors prove that the neural network used in the DGM converges to the exact solution of the PDE with an increasing number of hidden units of the neural network.

## Conclusion

Partial Differential Equations serve as the mathematical language that allows us to articulate and comprehend the complex dynamics of the world around us. As we push the boundaries of technology and scientific inquiry, innovative methods and interdisciplinary collaborations continue to reshape our approach to solving and understanding PDEs. In this blogpost about the paper *'DGM: A deep learning algorithm for solving partial differential equations'* by Justin Sirignano and Konstantinos Spiliopoulos I looked at the Deep Galerkin Method, a new approach for approximating the solution of a PDE.

The DGM presents a promising idea for efficiently solving high-dimensional PDEs without relying on a traditional mesh-based method. This makes the DGM computionally more tractable as the number of dimensions of the PDE increases. Its application to diverse problems and the theoretical foundation laid out in this post demonstrate its potential impact. The DGM employs a deep neural network in order to satisfy the differential operator, initial conditions, and boundary conditions of the PDE using stochastic gradient descent on randomly sampled time and spatial points.

Future research ideas made by the authors include exploring further improvements in computational efficiency and the extension of the method to different types of PDEs like hyperbolic, elliptic or partial-integral differential equations.

I hope this blogpost did its job of explaining the ideas presented in the paper intuitively as well as on a algorithmic level. The DGM in essence is a rather straightforward approach for approximating PDEs compared to some other methods (think of your class on scientific computing), but I believe that that just makes it more interesting (and of course the numerical results showcase that the found solution is indeed a good one). I am happy to receive some feedback and if there are any questions about the blogpost you can write me an E-Mail and I will try to answer it as fast as I can. Until then, thanks for reading!

## References

- Sirignano, J., Spiliopoulos, K. (2018). *DGM: A deep learning algorithm for solving partial differential equations*. [arXiv: 1708.07469](https://arxiv.org/abs/1708.07469)
