---
title: 'DGM: A deep learning algorithm for solving partial differential equations'
date: 2023-11-17
permalink: /posts/dgm
excerpt: "A blog post on the paper <i>'DGM: A deep learning algorithm for solving partial differential equations'</i> by Justin Sirignano and Konstantinos Spiliopoulos, published in 2018."
categories: 
  - Blog Posts
tags:
  - Deep Learning 
  - Machine Learning
  - Partial Differential Equation
---

Partial Differential Equations (PDEs) play a crucial role in modeling various phenomena in physics, engineering, and finance. Solving high-dimensional PDEs has been a longstanding computational challenge, with traditional methods becoming impractical due to the explosion in the number of grid points and the demand for reduced time step size.

In this blog post, I explore the approach from the paper *'DGM: A deep learning algorithm for solving partial differential equations'* by Justin Sirignano and Konstantinos Spiliopoulos, published in 2018 called the Deep Galerkin Method (DGM) that leverages deep learning to efficiently solve high-dimensional PDEs. 

## TL;DR
High-dimensional PDEs, with multiple spatial dimensions, pose a computational challenge due to the impracticality of using traditional mesh-based finite difference methods. The mesh size grows exponentially with the number of dimensions, making it computationally intractable. The paper proposes a mesh-free approach using a deep neural network to approximate a solution, mitigating the challenges associated with traditional methods.

The Deep Galerkin Method (DGM) is a mesh-free algorithm inspired by the Galerkin method, a widely-used computational technique for solving PDEs. However, instead of representing the solution as a linear combination of basis functions, the DGM employs a deep neural network. The neural network is trained to satisfy the differential operator, initial conditions, and boundary conditions using stochastic gradient descent on randomly sampled spatial points.

The authors present promising numerical results for the DGM on a class of high-dimensional free boundary PDEs, demonstrating its accuracy in solving problems up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton-Jacobi-Bellman PDE and Burgers' equation, showcasing its versatility and robustness.

To address the computational cost associated with calculating second derivatives in higher dimensions, the paper introduces a modified algorithm using a Monte Carlo method. This approach significantly reduces computational expenses while introducing some bias and variance. The trade-off between computational efficiency and accuracy is carefully considered.

In conclusion, by combining insights from machine learning and numerical methods, the Deep Galerkin Method opens new possibilities for tackling complex high-dimensional PDEs in various scientific and engineering domains.

## Introduction
Addressing the computational hurdles posed by high-dimensional Partial Differential Equations (PDEs) that involve multiple spatial dimensions is a challenging task. Traditional mesh-based finite difference methods become impractical in such scenarios, as the mesh size grows exponentially with the increasing number of dimensions, rendering computations intractable. In response to this challenge, the paper introduces an innovative solution that embraces a mesh-free approach, leveraging the power of deep neural networks to approximate solutions and alleviate the limitations associated with conventional methods.

At the core of this approach is the Deep Galerkin Method (DGM), a mesh-free algorithm inspired by the well-established Galerkin method, widely employed in computational techniques for solving PDEs. Diverging from the conventional representation of the solution as a linear combination of basis functions, the DGM takes a novel path by incorporating a deep neural network into its framework. This neural network undergoes training to conform to the requirements of the differential operator as well as the initial and boundary conditions through stochastic gradient descent on randomly sampled time and spatial points. This training process enhances the DGM's ability to provide accurate solutions while circumventing the computational challenges associated with traditional methodologies.

In this blog post I will first explain all the necessarry background information that is necessary to understand to needed concepts. Together, we will have a look at
- Partial Differential Equation
- (Stochastic) Gradient Descent
- Antithetic Sampling

Then I will show the algorithm presented in the paper as well as modification to it in order to increase the computation speed. And finally, I will have a look on the experimental results that were made by the authors.

## Partial Differential Equations
Partial Differential Equations (PDEs) stand as the backbone of mathematical modeling, governing phenomena from heat transfer and fluid dynamics to quantum mechanics. These intricate mathematical entities have been pivotal in our quest to understand and predict the behavior of complex systems.

Partial Differential Equations involve functions of multiple variables and their partial derivatives. They play a pivotal role in describing how physical quantities vary with respect to multiple independent variables, such as space and time. The general form of a PDE can be expressed as:

$$F(t, x_1, ..., x_n, u, \frac{\partial u}{\partial t}, \frac{\partial u}{\partial x_1}, ..., \frac{\partial u}{\partial x_n}, \frac{\partial^2 u}{\partial x_{1}^2}, ..., \frac{\partial^2 u}{\partial x_{n}^2},...) = 0$$

### Significance of PDEs

- Modeling Physical Phenomena: PDEs are indispensable in modeling various physical phenomena, such as heat conduction, fluid flow, electromagnetic fields, and quantum mechanics. They provide a mathematical framework to describe how these systems evolve over time and space.

- Engineering and Technology: Engineers leverage PDEs to optimize designs, simulate real-world scenarios, and enhance the performance of diverse systems, including bridges, aircraft, and electronic devices.

- Environmental Studies: PDEs are crucial in environmental studies, helping scientists model and understand complex processes like groundwater flow, air pollution dispersion, and climate dynamics.

### Challenges in Solving PDEs

- High Dimensionality: In many practical applications, PDEs involve a large number of variables, leading to high-dimensional systems. Traditional numerical methods can become computationally intractable in such cases.

- Boundary and Initial Conditions: Specifying appropriate boundary and initial conditions is essential for solving PDEs accurately. In some scenarios, these conditions may not be known precisely, posing a challenge in obtaining reliable solutions.

## A note on the Galerkin Method
The Galekin method named after the Russian mathematician Boris Galerkin and developed in 1915 is as a numerical approach to solving partial differential equations. It doesn't attempt to find an exact solution to a problem but instead seeks an approximate solution by projecting it onto a finite-dimensional subspace.

At its core, the Galekin method involves choosing a finite-dimensional space of basis functions and determining the coefficients that minimize the residual in the chosen space. With this approach it is possible to transform a rather challenging differential equation into a easier to solve algebraic problem, allowing for efficient computation and approximation.

The algorithm presented in the paper is called "Deep Galerkin Method" (DGM), since it is similar in spirit to the Galerkin method, in the sense that the algorithm tries to approximate the solution of the PDE. But instead of using of using a linear combination of basis functions the DGM uses a deep neural network to satisfy the differential operator, the initial condition and the boundary conditions pf the PDE. 

## On (Stochastic) Gradient Descent
- a

## The algorithm

- Define (general) PDE
- Define deep neural network with parameters $$\theta$$
- Algorithm:
  - Draw random points $$s_n$$
  - Define error function $$G(\theta_n, s_n)$$
  - Take descent steps until convergence: $$\theta_{n+1}= \theta_{n} - \alpha_n \nabla_{\theta}G(\theta_n, s_n)$$

- Show correctness

### Monte Carlo 2nd derivatives

- Problem with computation of second derivatives
- Describe Monte Carlo approach for fast computation of 2nd derivatives
- Show approximation bias, approximation error 
- Describe the modified algorithm

## Numerical Analysis

Here we discuss the numerical results of the algorithm.

The algorithm is tested on
- High-dimensional Free Boundary PDE
- High-dimensional Hamilton-Jacobi-Bellman PDE
- Burgersâ€™ equation

The discussion of the experiments is structured as 
- Define the PDE
- Adjust the algorithm to the specific problem
- Implementation details
- Results

## Neural Network Approximation Theorem for PDEs

In progress

The authors prove that the neural network used in the DGM converges to the exact solution of the PDE with an increasing number of hidden units of the neural network.

## Conclusion

Partial Differential Equations serve as the mathematical language that allows us to articulate and comprehend the complex dynamics of the world around us. As we push the boundaries of technology and scientific inquiry, innovative methods and interdisciplinary collaborations continue to reshape our approach to solving and understanding PDEs. In this blogpost about the paper *'DGM: A deep learning algorithm for solving partial differential equations'* by Justin Sirignano and Konstantinos Spiliopoulos I looked at the Deep Galerkin Method, a new approach for approximating the solution of a PDE.

The DGM presents a promising idea for efficiently solving high-dimensional PDEs without relying on a traditional mesh-based method. This makes the DGM computionally more tractable as the number of dimensions of the PDE increases. Its application to diverse problems and the theoretical foundation laid out in this post demonstrate its potential impact. The DGM employs a deep neural network in order to satisfy the differential operator, initial conditions, and boundary conditions of the PDE using stochastic gradient descent on randomly sampled time and spatial points.

Future research ideas made by the authors include exploring further improvements in computational efficiency and the extension of the method to different types of PDEs like hyperbolic, elliptic or partial-integral differential equations.

I hope this blogpost did its job of explaining the ideas presented in the paper intuitively as well as on a algorithmic level. The DGM in essence is a rather straightforward approach for approximating PDEs compared to some other methods (think of your class on scientific computing), but I believe that that just makes it more interesting (and of course the numerical results showcase that the found solution is indeed a good one). I am happy to receive some feedback and if there are any questions about the blogpost you can write me an E-Mail and I will try to answer it as fast as I can. Until then, thanks for reading!

## References

- Sirignano, J., Spiliopoulos, K. (2018). *DGM: A deep learning algorithm for solving partial differential equations*. [arXiv: 1708.07469](https://arxiv.org/abs/1708.07469)
